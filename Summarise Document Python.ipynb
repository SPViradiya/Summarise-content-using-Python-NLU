{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "I have created one class named \"SummarizeDocument\". It has different methods as per requirement. Methods are as follow:\n",
    "<ol>\n",
    "    <li><b>get_data:</b> To fetch the information from the given URL</li>\n",
    "    <li><b>process_clean_data:</b> To process the data, Remove HTML tags etc. It uses clean_data method</li>\n",
    "    <li><b>clean_data:</b> To clean the data, remove unwanted brackets with text as we have in WikiPedia, Replace some UTF-8 encoding characters</li>\n",
    "    <li><b>tokenize_data:</b> To create bad of word</li>\n",
    "    <li><b>get_word_frequency:</b> To count occurance of each words</li>\n",
    "    <li><b>norm_min_max:</b> Normalization using max and min of word frequency</li>\n",
    "    <li><b>norm_mean_sd:</b> Normalization using mean and standard deviation of word frequency</li>\n",
    "    <li><b>get_sentences:</b> To create sentences from article</li>\n",
    "    <li><b>get_sentence_weight:</b> To give weight to sentences</li>\n",
    "    <li><b>get_summary:</b> To get summary of the article. It will print some sentences</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the libraries\n",
    "\n",
    "import string\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from statistics import mean, stdev\n",
    "import heapq\n",
    "\n",
    "class SummarizeDocument(object):\n",
    "    \n",
    "    #Constructor\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words('english')) # All possible stopwords\n",
    "\n",
    "    # url (string): URL from where you want to fetch (crawl) the data\n",
    "    def get_data(self, url):\n",
    "        crawl_data = urllib.request.urlopen(wiki_url)\n",
    "        html_content = crawl_data.read().decode(\"utf-8\")\n",
    "        \n",
    "        return BeautifulSoup(html_content, \"html.parser\").find_all(\"p\")\n",
    "        \n",
    "    # data (list) : Unparsed data from beautiful soup\n",
    "    def process_clean_data(self, data):\n",
    "        # clean the data\n",
    "        processed_data = [self.clean_data(p.text) for p in data]\n",
    "\n",
    "        return \" \".join(processed_data).strip()\n",
    "        \n",
    "    # We can use regular expression in order to remove html content. I used it before and it worked well but later on\n",
    "    # I found out that we have .text method in BeautifulSoup which exactly does the same\n",
    "    # data (list) : Unparsed data from beautiful soup\n",
    "    def clean_data(self, data):\n",
    "        #cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "        #cleantext = re.sub(cleanr, '', raw_html)\n",
    "        #Above both line can be replaced by .text function in BeautifulSoup. It is done in get_data function\n",
    "\n",
    "        #Remove anything that comes between square brackets\n",
    "        raw_data = re.sub(r'\\[.*?\\]', '', data)\n",
    "\n",
    "        #Replaceing \\xa0 which is actually non-breaking space in Latin1 (ISO 8859-1)\n",
    "        data = raw_data.replace(u'\\xa0', u' ')\n",
    "        return data.replace('\\n','') # Removing \\n from data\n",
    "    \n",
    "    # Tokenize the data\n",
    "    # data (list) : Parsed and clean data\n",
    "    def tokenize_data(self, data):\n",
    "        all_tokens = word_tokenize(data)\n",
    "\n",
    "        #Remove words like 's or 'll or 'it.\n",
    "        #Remove single quote before word like we have 'without, 'and etc.\n",
    "        for k,tok in enumerate(all_tokens):\n",
    "            if re.match(r\"\\'[A-Za-z0-9]{1,2}$\",tok):\n",
    "                all_tokens.remove(tok)\n",
    "            elif re.match(r\"\\'[A-Za-z0-9]{3,}\",tok):\n",
    "                all_tokens[k] = tok.replace(\"'\",\"\")\n",
    "\n",
    "        return [tok.lower() for tok in all_tokens if tok.lower() not in string.punctuation and tok.lower() not in self.stopwords]\n",
    "\n",
    "    # Count the frequency of every word\n",
    "    # data (list | dictionary) : Parsed and clean data\n",
    "    def get_word_frequency(self, data):\n",
    "        freq_table = defaultdict(int)\n",
    "        for word in data:\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        return freq_table\n",
    "    \n",
    "    # Normalization mentod - 1 using max and min \n",
    "    # word_count (dictionary) : Dictionary having word frequency \n",
    "    def norm_min_max(self, word_count):\n",
    "        all_counts = word_count.values() # All the count from bag of words\n",
    "        M = max(all_counts) # Mamimum count\n",
    "        m = min(all_counts) # Min count\n",
    "        range_data = M - m # Range of count\n",
    "\n",
    "        weight = defaultdict(int)\n",
    "\n",
    "        for word, cnt in word_count.items():\n",
    "            weight[word] = cnt / range_data\n",
    "\n",
    "        return weight\n",
    "    \n",
    "    # Normalization mentod - 2 using mean and standard deviation\n",
    "     # word_count (dictionary) : Dictionary having word frequency \n",
    "    def norm_mean_sd(self, word_count):\n",
    "        all_counts = word_count.values() # All the count from bag of words\n",
    "        mean_val = mean(all_counts)\n",
    "        stdev_val = stdev(all_counts)\n",
    "\n",
    "        weight = defaultdict(int)\n",
    "\n",
    "        for word, cnt in word_count.items():\n",
    "            weight[word] = (cnt - mean_val) / stdev_val\n",
    "\n",
    "        return weight\n",
    "    \n",
    "    \n",
    "    # Convert the data into sentences\n",
    "    # data (list) : Parsed and clean data\n",
    "    def get_sentences(self, data):\n",
    "        return sent_tokenize(data)\n",
    "    \n",
    "    # Give weight to the sentece\n",
    "    # sentences (list) : List with all sentences of document\n",
    "    # word_with_weight (dictionary): All word having weights using normalization\n",
    "    def get_sentence_weight(self, sentences, word_with_weight):\n",
    "        sentence_weight = {}\n",
    "        for sent in sentences:\n",
    "            word_tokens = word_tokenize(sent)\n",
    "            sent_length = len(sent.split(\" \"))\n",
    "\n",
    "            if sent_length < 30:\n",
    "                for word in word_tokens:\n",
    "                    if word in word_with_weight:\n",
    "                        if sent in sentence_weight:\n",
    "                            sentence_weight[sent] += word_with_weight[word]\n",
    "                        else:\n",
    "                            sentence_weight[sent] = word_with_weight[word]\n",
    "                            \n",
    "        return sentence_weight\n",
    "    \n",
    "    # Get summary of document\n",
    "    # num_of_sent (int) : Number of sentences required as summary (This is top K)\n",
    "    # weighted_sentences (dictionaty) : All senteces with respective weight\n",
    "    def get_summary(self, num_of_sent, weighted_sentences):\n",
    "        summary_sentences = heapq.nlargest(num_of_sent, weighted_sentences, key=weighted_sentences.get)\n",
    "        print(\" \".join(summary_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covid 19\n",
    "### Normalization using max and min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During the initial outbreak in Wuhan, China, the virus and disease were commonly referred to as \"coronavirus\" and \"Wuhan coronavirus\", with the disease sometimes called \"Wuhan pneumonia\". People are most infectious when they show symptoms (even mild or non-specific symptoms), but may be infectious for up to two days before symptoms appear (pre-symptomatic transmission). It is most contagious during the first three days after the onset of symptoms, although spread is possible before symptoms appear, and from people who do not show symptoms. Coronavirus disease 2019 (COVID-19) is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Following the infection, children may develop paediatric multisystem inflammatory syndrome, which has symptoms similar to Kawasaki disease, which can be fatal. The disease may take a mild course with few or no symptoms, resembling other common upper respiratory diseases such as the common cold. In a study of early cases, the median time from exhibiting initial symptoms to death was 14 days, with a full range of six to 41 days. The WHO additionally uses \"the COVID‑19 virus\" and \"the virus responsible for COVID‑19\" in public communications. The time from exposure to onset of symptoms is typically around five days but may range from two to fourteen days. Greater prevalence of lacking health insurance and care and of underlying conditions such as diabetes, hypertension and heart disease also increase their risk of death. An acute cardiac injury was found in 12% of infected people admitted to the hospital in Wuhan, China, and is more frequent in severe disease. Early symptoms may include a wide variety of symptoms but infrequently involves shortness of breath. A study of the first 41 cases of confirmed COVID‑19, published in January 2020 in The Lancet, revealed the earliest date of onset of symptoms as 1 December 2019. Both the disease and virus are commonly referred to as \"coronavirus\" in the media and public discourse.\n"
     ]
    }
   ],
   "source": [
    "#Initialization of SummarizeDocument\n",
    "doc_summ = SummarizeDocument()\n",
    "\n",
    "# Fetching the data from WikiPedia\n",
    "unparsed_text = doc_summ.get_data(\"https://en.wikipedia.org/wiki/Coronavirus_disease_2019\")\n",
    "\n",
    "# Processing and cleaning the data\n",
    "parsed_clean_text = doc_summ.process_clean_data(unparsed_text)\n",
    "\n",
    "# Creating tokens\n",
    "bag_of_words = doc_summ.tokenize_data(parsed_clean_text)\n",
    "\n",
    "# Get word frequencies\n",
    "word_frequency = doc_summ.get_word_frequency(bag_of_words)\n",
    "\n",
    "# Normalization\n",
    "word_with_weight = doc_summ.norm_min_max(word_frequency)\n",
    "\n",
    "# Creating sentences from article\n",
    "all_sentences = doc_summ.get_sentences(parsed_clean_text)\n",
    "\n",
    "# Giving weight to each sentences based on the words it have\n",
    "sent_with_weight = doc_summ.get_sentence_weight(all_sentences, word_with_weight)\n",
    "\n",
    "# As per requirement, K sentences containing 5% of words. 1000 / 20 = 50 where 50 is 5%\n",
    "\n",
    "# Algotithem to find K\n",
    "# We need to find the number of the sentence. \n",
    "#     1) Find averge length of the words in the sentences (That will be our expected sentence length)\n",
    "#     2) Find number of word that should be in summary. ( Divide total word count by 20)\n",
    "#     3) Divide 2nd step count / 1st step count and that will be the number of sentence\n",
    "\n",
    "# First step\n",
    "total_words = len(word_tokenize(parsed_clean_text))\n",
    "total_sentences = len(all_sentences)\n",
    "average_sentence_length = total_words/total_sentences\n",
    "\n",
    "# Second step\n",
    "summary_no_of_words = total_words / 20\n",
    "\n",
    "# Third step\n",
    "K = round(summary_no_of_words / average_sentence_length)\n",
    "\n",
    "doc_summ.get_summary(K, sent_with_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization using mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During the initial outbreak in Wuhan, China, the virus and disease were commonly referred to as \"coronavirus\" and \"Wuhan coronavirus\", with the disease sometimes called \"Wuhan pneumonia\". People are most infectious when they show symptoms (even mild or non-specific symptoms), but may be infectious for up to two days before symptoms appear (pre-symptomatic transmission). It is most contagious during the first three days after the onset of symptoms, although spread is possible before symptoms appear, and from people who do not show symptoms. Coronavirus disease 2019 (COVID-19) is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Following the infection, children may develop paediatric multisystem inflammatory syndrome, which has symptoms similar to Kawasaki disease, which can be fatal. The disease may take a mild course with few or no symptoms, resembling other common upper respiratory diseases such as the common cold. The WHO additionally uses \"the COVID‑19 virus\" and \"the virus responsible for COVID‑19\" in public communications. Early symptoms may include a wide variety of symptoms but infrequently involves shortness of breath. In a study of early cases, the median time from exhibiting initial symptoms to death was 14 days, with a full range of six to 41 days. The time from exposure to onset of symptoms is typically around five days but may range from two to fourteen days. An acute cardiac injury was found in 12% of infected people admitted to the hospital in Wuhan, China, and is more frequent in severe disease. Both the disease and virus are commonly referred to as \"coronavirus\" in the media and public discourse. Greater prevalence of lacking health insurance and care and of underlying conditions such as diabetes, hypertension and heart disease also increase their risk of death. Wuhan, China), animal species or groups of people in disease and virus names to prevent social stigma.\n"
     ]
    }
   ],
   "source": [
    "#Initialization of SummarizeDocument\n",
    "doc_summ = SummarizeDocument()\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Coronavirus_disease_2019\"\n",
    "\n",
    "# Fetching the data from WikiPedia\n",
    "unparsed_text = doc_summ.get_data(url)\n",
    "\n",
    "# Processing and cleaning the data\n",
    "parsed_clean_text = doc_summ.process_clean_data(unparsed_text)\n",
    "\n",
    "# Creating tokens\n",
    "bag_of_words = doc_summ.tokenize_data(parsed_clean_text)\n",
    "\n",
    "# Get word frequencies\n",
    "word_frequency = doc_summ.get_word_frequency(bag_of_words)\n",
    "\n",
    "# Normalization\n",
    "word_with_weight = doc_summ.norm_mean_sd(word_frequency)\n",
    "\n",
    "# Creating sentences from article\n",
    "all_sentences = doc_summ.get_sentences(parsed_clean_text)\n",
    "\n",
    "# Giving weight to each sentences based on the words it have\n",
    "sent_with_weight = doc_summ.get_sentence_weight(all_sentences, word_with_weight)\n",
    "\n",
    "# As per requirement, K sentences containing 5% of words. 1000 / 20 = 50 where 50 is 5%\n",
    "\n",
    "# Algotithem to find K\n",
    "# We need to find the number of the sentence. \n",
    "#     1) Find averge length of the words in the sentences (That will be our expected sentence length)\n",
    "#     2) Find number of word that should be in summary. ( Divide total word count by 20)\n",
    "#     3) Divide 2nd step count / 1st step count and that will be the number of sentence\n",
    "\n",
    "# First step\n",
    "total_words = len(word_tokenize(parsed_clean_text))\n",
    "total_sentences = len(all_sentences)\n",
    "average_sentence_length = total_words/total_sentences\n",
    "\n",
    "# Second step\n",
    "summary_no_of_words = total_words / 20\n",
    "\n",
    "# Third step\n",
    "K = round(summary_no_of_words / average_sentence_length)\n",
    "\n",
    "doc_summ.get_summary(K, sent_with_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Both normalization have created almost same summarization. Only last one or two sentences are differing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
